
<!-- ## 程序和内存位置 -->
## Programs and memory location

<!-- 我们已经实现了一个多道程序的多任务系统,
但却在尝试运行第二个用户进程的时候遇到了问题.
如果你还记得编译Navy-apps中的程序时, 我们都把它们链接到`0x3000000`或`0x83000000`的内存位置,
你就会知道问题的原因了: 我们在加载第二个用户进程的时候, 就会覆盖第一个进程的内容! -->
We have implemented a multi-tasking system for multi-programming. However, we encountered an issue when attempting to run the second user process. If you recall, when compiling programs in Navy-apps, we linked them to memory addresses `0x3000000` or `0x83000000`. This is the reason for the problem: when loading the second user process, we overwrite the content of the first process!

<!-- 出现上述问题的根本原因是, "在内存中可以同时存在多个进程"的条件被打破了.
显然我们需要对内存进行管理, 而不是让多个进程随意使用.
我们在PA3中提到操作系统有管理系统资源的义务,
在多任务操作系统中, 内存作为一种资源自然也要被管理起来.
操作系统需要记录内存的分配情况, 需要运行一个新进程的时候,
就给它分配一片空闲的内存位置, 把它加载到这一内存位置上即可. -->
The fundamental reason for the previous issue is that the condition of "multiple processes coexisting in memory simultaneously" is being violated. Clearly, we need to manage memory instead of allowing multiple processes to use it freely. As we mentioned in PA3, an operating system has the obligation to manage system resources. In a multitasking operating system, memory, as a resource, naturally needs to be managed. The operating system must keep track of memory allocation. When it's time to run a new process, it should allocate an unused memory area and load the process into that memory location.

<!-- 不过, 这片空闲的内存位置是操作系统的加载器在加载时刻指定的,
但进程代码真的可以在这一内存位置上正确运行吗? -->
However, the free memory location is specified by the operating system's loader at the time of loading. But can the process code really run correctly at this memory location?

<!-- ### 绝对代码 -->
Absolute code

<!-- 一般来说, 程序的内存位置是在[链接时刻(link time)][link time]确定的(Navy-apps中的程序就是这样),
以前的程序员甚至在程序中使用绝对地址来进行内存访问,
这两种代码称为绝对代码(absolute code).
绝对代码会假设程序中的对象(函数和数据)位于某个固定的位置,
比如链接时重定位之后, 程序就会认为某个变量`a`位于内存位置`0x83001234`,
如果我们把程序加载到别的位置, 加载后变量`a`的实际位置就改变了,
但程序中的绝对代码还是认为变量`a`仍然位于内存位置`0x83001234`,
访问`0x83001234`也就无法访问真正的变量`a`.
显然, 绝对代码只能在固定的内存位置才能正确运行. -->
Typically, the memory location of a program is determined at [link time][link time] (as is the case with programs in Navy-apps). Some earlier programmers even used absolute addresses for memory access within their programs. These two types of code are referred to as absolute code. Absolute code assumes that objects (functions and data) in the program reside at fixed locations. For instance, after link-time relocation, the program might assume that a variable `a` resides at memory location `0x83001234`. If we load the program into a different location, the actual location of variable `a` changes accordingly. However, the absolute code within the program still assumes that variable `a` resides at memory location `0x83001234`, thus attempting to access `0x83001234` would not access the actual variable `a`. Clearly, absolute code can only run correctly at fixed memory locations.

[link time]: https://en.wikipedia.org/wiki/Link_time

<!-- 但操作系统在加载时刻分配的空闲内存位置, 并不总是能让这种程序正确运行.
因此, 这个问题的一个解决方案, 就是让操作系统记录程序的加载位置,
当一个程序试图加载到一个已经被使用的内存位置时, 加载将会失败, 操作系统将返回一个错误.
为了避免加载失败, 一个方法是为每个程序维护多个不同加载地址的版本,
期望其中有一个版本可以被成功加载. -->
However, the vacant memory location allocated by the operating system at load time does not always guarantee correct execution for such programs. Therefore, one solution to this problem is for the operating system to keep track of the program's load location. When a program attempts to load into a memory location that is already in use, the loading will fail, and the operating system will return an error. To avoid loading failures, one approach is to maintain multiple versions of the program with different load addresses, hoping that one version can be successfully loaded.

<!-- ### 可重定位代码 -->
Relocatable code

<!-- 但这样太麻烦了. 为什么一定要提前确定一个程序的加载位置呢?
如果我们把链接时的重定位阶段往后推迟, 不就可以打破绝对代码的限制了吗? -->
But this is too cumbersome. Why do we have to determine a program's load position in advance? If we delay the relocation phase until later, during linking, wouldn't we break the limitations of absolute code?

于是有程序员开发了一类"[自重定位(self-relocation)][self relocation]"的特殊程序,
这种程序可以在开始运行的时候, 先把自己重定位到其它内存位置, 然后再开始真正的运行.
这种重定位类型称为"[运行时(run time)][run time]重定位".
So, some programmers developed a category of special programs called "[self-relocating][self relocation]" programs. These programs, at the beginning of execution, relocate themselves to another memory location before starting actual execution. This type of relocation is called "[run-time][run time] relocation."

<!-- 我们在PA1中提到了BIOS这个概念, 事实上, BIOS中的程序就是这种自重定位的程序.
这是因为, BIOS一般是固化在ROM中, 无法对ROM中的数据进行写操作.
计算机启动后会从BIOS中执行程序, 但这个BIOS程序马上会把自己拷贝到其它内存位置,
并进行一些重定位工作, 然后才真正开始执行.
这时候, 数据就位于可写的RAM中了, BIOS程序就可以对其进行写操作. -->
In PA1, we mentioned the concept of BIOS. In fact, programs in BIOS are precisely this type of self-relocating programs. This is because BIOS is typically embedded in ROM, and data in ROM cannot be written to. When the computer starts up, it executes programs from the BIOS. However, the BIOS program immediately copies itself to another memory location, performs some relocation work, and then truly begins execution. At this point, the data resides in writable RAM, allowing the BIOS program to perform write operations on it.

<!-- 但对多任务操作系统来说, 这并没有真正解决问题,
因为程序在运行时刻并不知道重定位的目标内存位置是否空闲. -->
But for a multitasking operating system, this doesn't truly solve the problem because the program at runtime doesn't know if the target memory location for relocation is free.

[self relocation]: https://en.wikipedia.org/wiki/Self-relocation
[run time]: https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)

<!-- 既然只有操作系统才知道内存是否空闲, 那就干脆让加载器来进行重定位吧,
于是有了"[加载时(load time)][load time]重定位"的说法.
具体地, 加载器会申请一个空闲的内存位置, 然后将程序加载到这个内存位置,
并把程序重定位到这个内存位置, 之后才会执行这个程序.
今天的GNU/Linux就是通过这种方式来插入[内核模块][kernel module]的. -->
Since only the operating system knows whether memory is free, let's just have the loader perform relocation. This gave rise to the concept of "[load-time][load time] relocation." Specifically, the loader will allocate a free memory location, load the program into that location, relocate the program to that memory location, and then execute the program. This is how GNU/Linux inserts [kernel modules][kernel module] today.

[load time]: https://en.wikipedia.org/wiki/Loader_(computing)
[kernel module]: https://en.wikipedia.org/wiki/Loadable_kernel_module

<!-- ### 位置无关代码 -->
### Position-independent code

<!-- 从某种程度上来说, 加载时重定位会带来额外的开销:
如果一个程序要被重复执行多次, 那么就要进行多次的加载时重定位.
早期的计算机速度较慢, 大家觉得加载时重定位的开销还是不能忽略的,
而且加载时重定位需要对整个程序进行处理,
如果程序比较大的话, 开销就更明显了. -->
To some extent, load-time relocation introduces additional overhead: if a program needs to be executed multiple times, it will require multiple load-time relocations. In the early days of computing, when computers were slower, the overhead of load-time relocation was considered significant and could not be ignored. Additionally, load-time relocation requires processing the entire program. If the program is large, the overhead becomes even more pronounced.

<!-- 有没有方法可以节省重定位的开销, 甚至不进行重定位呢?
但链接时的重定位又可能会产生绝对代码, 这并不是我们所希望的.
如果程序中的所有寻址, 都是针对程序位置来进行相对寻址操作,
这样的程序就可以被加载到任意位置执行, 而不会出现绝对代码的问题了, -->
Is there a way to save the overhead of relocation or even avoid relocation altogether? However, link-time relocation may result in absolute code, which is not desirable. If all addressing within the program is done relative to the program's location, the program can be loaded and executed at any position without encountering issues with absolute code.

<!-- 这就是[PIC(position-independent code, 位置无关代码)][pic]的基本思想.
今天的动态库都是PIC, 这样它们就可以被加载到任意的内存位置了.
此外, 如果一个可执行文件全部由PIC组成, 那么它有一个新名字,
叫[PIE(position-independent executable, 位置无关可执行文件)][pie].
编译器可以通过特定的选项编译出PIE.
和一般的程序不同, PIE还能在一定程度上对恶意的攻击程序造成了干扰:
恶意程序也无法提前假设PIE运行的地址. -->
This is the basic idea of Position-Independent Code ([PIC][pic]). Nowadays, dynamic libraries are all PIC, enabling them to be loaded into any memory location. Additionally, if an executable file consists entirely of PIC, it has a new name called Position-Independent Executable ([PIE][pie]). Compilers can generate PIE by specific options. Unlike regular programs, PIE also provides some interference against malicious attack programs to some extent: malicious programs cannot assume the address where PIE will run in advance.
<!-- 也正是因为这一安全相关的特性, 最近的不少GNU/Linux的发行版上配套的gcc都默认生成PIE.
不过, 使用相对寻址会使得程序的代码量增大, 性能也会受到一些影响,
但对于早期的计算机来说, 内存是一种非常珍贵的资源,
降低性能也是大家不愿意看到的, 因此对于PIC和PIE, 大家也会慎重考虑. -->
Also, due to this security-related feature, many recent GNU/Linux distributions come with gcc configured to default to generating Position-Independent Executables (PIE). However, using relative addressing can increase the code size of the program and may have some performance implications. But for early computers, where memory was a precious resource, any performance reduction was undesirable. Therefore, when considering Position-Independent Code (PIC) and Position-Independent Executables (PIE), people tend to weigh their options carefully.

[pic]: https://en.wikipedia.org/wiki/Position-independent_code
[pie]: https://en.wikipedia.org/wiki/Position-independent_code#PIE

<!-- > #### question::实现基于PIE的loader (建议二周目思考)
> 天下并没有免费的午餐, PIE之所以能做到位置无关,
> 其实是要依赖于程序中一个叫[GOT(global offset table, 全局偏移量表)][got]的数据结构.
> 要正确运行PIE, 加载器需要在加载程序的时候往GOT中填写正确的内容.
>
> 有兴趣的同学可以让Nanos-lite的loader支持PIE, 当然这需要了解一些ELF相关的细节,
> 具体细节可以参考ABI手册. -->
> #### question::Implementing a PIE-based loader (suggested second round consideration)
> There's no such thing as a free lunch. The reason PIE achieves position independence is actually relying on a data structure within the program called the [Global Offset Table (GOT)][got]. To run PIE correctly, the loader needs to fill the GOT with the correct content when loading the program.
> 
> Students who are interested can make the Nanos-lite loader support PIE. Of course, this requires understanding some details related to ELF, for which specific details can be found in the ABI manual.

[got]: https://www.technovelty.org/linux/plt-and-got-the-key-to-code-sharing-and-dynamic-libraries.html

<!-- 如果世界上的所有程序都是可重定位的, 或者是PIC, 内存覆盖的问题就不攻自破了.
但总有一些包含绝对代码的程序, 考虑到兼容问题, 还需要想办法运行它们.
有没有更好的, 一劳永逸的方案呢? -->
If all programs in the world were relocatable or Position-Independent Code (PIC), the problem of memory overlay would be solved. However, there are always some programs that contain absolute code. Considering compatibility issues, is there a better, more permanent solution?

<!-- ## 虚实交错的魔法 -->
## The Magic of Intertwining Reality and Fantasy


<!-- 我们刚才是从程序本身来考量, 自然无法绕开绝对代码的问题.
为了解决这个问题, 我们需要从另一个方面 - 内存 - 来思考.
我们知道程序会经历编译, 链接, 加载, 运行这四个阶段,
绝对代码经过编译链接之后, 程序看到的内存地址就会确定下来了,
加载运行的时候就会让程序使用这一内存地址, 来保证程序可以正确运行.
一种尝试是把程序看到的内存和它运行时候真正使用的内存解耦开来.
这就是虚拟内存的思想. -->
We just considered the program itself, so we couldn't avoid the issue of absolute code.
To solve this problem, we need to think from another perspective - memory - .
We know that a program goes through four stages: compilation, linking, loading, and running.
After the absolute code is compiled and linked, the memory addresses seen by the program are determined.
During loading and running, the program uses these memory addresses to ensure correct operation.
One attempt is to decouple the memory seen by the program from the memory it actually uses at runtime.
This is the idea behind virtual memory.

<!-- 所谓虚拟内存, 就是在真正的内存(也叫物理内存)之上的一层专门给进程使用的抽象.
有了虚拟内存之后, 进程只需要认为自己运行在虚拟地址上就可以了,
真正运行的时候, 才把虚拟地址映射到物理地址.
这样, 我们只要把程序链接到一个固定的虚拟地址,
加载的时候把它们加载到不同的物理地址,
并维护好虚拟地址到物理地址的映射关系, 就可以一劳永逸地解决上述问题了! -->
Virtual memory is an abstraction layer above real memory (also known as physical memory) that is dedicated to processes. With virtual memory, a process only needs to think it is running on virtual addresses. When actually running, the virtual addresses are mapped to physical addresses. By linking a program to a fixed virtual address and loading it into different physical addresses, and maintaining the mapping between virtual addresses and physical addresses, we can permanently solve the above problems!

<!-- 那么, 在进程运行的时候, 谁来把虚拟地址映射成物理地址呢?
我们在PA1中已经了解到指令的生命周期: -->
So, during the execution of a process, who maps the virtual address to the physical address?
We have already learned about the lifecycle of an instruction in PA1:
<!-- ```c
while (1) {
  从PC指示的存储器位置取出指令;
  执行指令;
  更新PC;
}
``` -->
```c
while (1) {
  Fetch the instruction from the memory location indicated by PC;
  Execute the instruction;
  Update PC;
}
```
<!-- 如果引入了虚拟内存机制, PC就是一个虚拟地址了,
我们需要在访问存储器之前完成虚拟地址到物理地址的映射.
尽管操作系统管理着计算机中的所有资源, 在计算机看来它也只是一个程序而已.
作为一个在计算机上执行的程序而言, 操作系统必定无法干涉指令执行的具体过程.
所以让操作系统来把虚拟地址映射成物理地址, 是不可能实现的. -->
If a virtual memory mechanism is introduced, then PC becomes a virtual address.
We need to complete the mapping from virtual address to physical address before accessing the memory.
Although the operating system manages all the resources in the computer, from the computer's perspective, it is just another program.
As a program running on the computer, the operating system cannot interfere with the specific process of instruction execution.
Therefore, it is impossible for the operating system to map virtual addresses to physical addresses.
<!-- 因此, 在硬件中进行这一映射是唯一的选择了:
我们在处理器和存储器之间添加一个新的硬件模块MMU(Memory Management Unit, 内存管理单元),
它是虚拟内存机制的核心, 肩负起这一机制最重要的地址映射功能.
需要说明的是, 我们刚才提到的"MMU位于处理器和存储器之间"只是概念上的说法.
事实上, 虚拟内存机制在现代计算机中是如此重要, 以至于MMU在物理上都实现在处理器芯片内部了. -->
Therefore, performing this mapping in hardware is the only choice:
We add a new hardware module MMU (Memory Management Unit) between the processor and memory.
It is the core of the virtual memory mechanism and carries the most important address mapping function of this mechanism.
It should be noted that the statement "MMU is located between the processor and memory" is only a conceptual description.
In fact, the virtual memory mechanism is so important in modern computers that the MMU is physically implemented inside the processor chip.

<!-- 但是, 只有操作系统才知道具体要把虚拟地址映射到哪些物理地址上.
所以, 虚拟内存机制是一个软硬协同才能工作的机制:
操作系统负责进行物理内存的管理,
加载进程的时候决定要把进程的虚拟地址映射到哪些物理地址;
等到进程真正运行之前, 还需要配置MMU, 把之前决定好的映射落实到硬件上,
进程运行的时候, MMU就会进行地址转换, 把进程的虚拟地址映射到操作系统希望的物理地址.
注意到这个映射是进程相关的: 不同的进程有不同的映射,
这意味着对不同的进程来说, 同一个虚拟地址可能会被映射到不同的物理地址.
这恰好一劳永逸地解决了内存覆盖的问题.
绝大部分多任务操作系统就是这样做的. -->
However, only the operating system knows the specific physical addresses to which virtual addresses should be mapped.
Therefore, the virtual memory mechanism is a mechanism that can only work through collaboration between software and hardware:
The operating system is responsible for managing physical memory and deciding which physical addresses to map the virtual addresses of a process to when loading the process.
Before the process actually runs, the MMU must be configured to implement the previously determined mapping in hardware.
When the process runs, the MMU will perform address translation, mapping the virtual addresses of the process to the physical addresses desired by the operating system.
It is worth noting that this mapping is process-specific: different processes have different mappings,
which means that for different processes, the same virtual address may be mapped to different physical addresses.
This neatly and permanently solves the problem of memory overlap.
Most multitasking operating systems work in this way.

<!-- ### 分段 -->
### Segmentation

<!-- 关于MMU具体如何进行地址映射, 目前主要有两种主流的方式.
最简单的方法就是, 物理地址=虚拟地址+偏移量.
这种最朴素的方式就是段式虚拟内存管理机制, 简称分段机制.
从直觉上来理解, 就是把物理内存划分成若干个段,
不同的进程就放到不同的段中运行, 进程不需要关心自己具体在哪一个段里面,
操作系统只要让不同的进程使用不同的偏移量, 进程之间就不会相互干扰了. -->
Regarding how the MMU specifically performs address mapping, there are currently two main mainstream methods.
The simplest method is that the physical address equals the virtual address plus an offset.
This most basic method is known as the segment-based virtual memory management mechanism, or simply segmentation.
Intuitively, this means dividing physical memory into several segments.
Different processes are placed in different segments to run. The processes do not need to worry about which segment they are in.
As long as the operating system uses different offsets for different processes, there will be no interference between them.

<!-- 分段机制在硬件上的实现可以非常简单, 只需要在MMU中实现一个段基址寄存器就可以了.
操作系统在运行不同进程的时候, 就在段基址寄存器中设置不同的值,
MMU会把进程使用的虚拟地址加上段基址, 来生成真正用于访问内存的物理地址,
这样就实现了"让不同的进程使用不同的段"的目的.
作为教学操作系统的Minix就是这样工作的,
一些简单的嵌入式系统和实时系统, 也是通过分段机制来进行虚存管理. -->
The implementation of the segmentation mechanism in hardware can be very simple, requiring only the implementation of a segment base register in the MMU.
When the operating system runs different processes, it sets different values in the segment base register.
The MMU will add the segment base to the virtual address used by the process to generate the actual physical address used to access memory.
This achieves the goal of "using different segments for different processes."
The teaching operating system Minix works in this way,
and some simple embedded systems and real-time systems also manage virtual memory through the segmentation mechanism.

<!-- 实际上, 处理器中的分段机制有可能复杂得多.
例如i386由于历史原因, 为了兼容它的前身8086, 不得已引入了段描述符,
段选择符, 全局描述符表(GDT), 全局描述符表寄存器(GDTR)等概念,
段描述符中除了段基址之外, 还描述了段的长度, 类型, 粒度, 访问权限等等的属性,
为了弥补段描述符的性能问题, 又加入了描述符cache等概念...
我们可以目睹一下i386分段机制的风采: -->
In reality, the segmentation mechanism in processors can be much more complex.
For example, the i386, due to historical reasons and for compatibility with its predecessor, the 8086, was forced to introduce concepts such as segment descriptors, segment selectors, Global Descriptor Table (GDT), Global Descriptor Table Register (GDTR), etc.
Besides the segment base, the segment descriptor also describes attributes such as the segment's length, type, granularity, access permissions, and so on.
To compensate for the performance issues of segment descriptors, concepts like descriptor cache were added...
We can witness the elegance of the i386 segmentation mechanism:
```
           15              0    31                                   0
  LOGICAL +----------------+   +-------------------------------------+
  ADDRESS |    SELECTOR    |   |                OFFSET               |
          +---+---------+--+   +-------------------+-----------------+
       +------+         V                          |
       | DESCRIPTOR TABLE                          |
       |  +------------+                           |
       |  |            |                           |
       |  |            |                           |
       |  |            |                           |
       |  |            |                           |
       |  |------------|                           |
       |  |  SEGMENT   | BASE          +---+       |
       +->| DESCRIPTOR |-------------->| + |<------+
          |------------| ADDRESS       +-+-+
          |            |                 |
          +------------+                 |
                                         V
              LINEAR  +------------+-----------+--------------+
              ADDRESS |    DIR     |   PAGE    |    OFFSET    |
                      +------------+-----------+--------------+
```
<!-- 咋看之下真是眼花缭乱, 让人一头雾水. -->
It is quite confusing at first glimpse.

<!-- 在NEMU中, 我们需要了解什么呢? 什么都不需要.
现在的大部分操作系统都不再使用分段机制, 就连i386手册中也提到可以想办法"绕过"它来提高性能:
将段基地址设成0, 长度设成4GB, 这样看来就像没有段的概念一样, 这就是i386手册中提到的"扁平模式".
当然, 这里的"绕过"并不是简单地将分段机制关掉(事实上也不可能关掉),
我们在PA3中提到的i386保护机制中关于特权级的概念, 其实就是i386分段机制提供的, 抛弃它是十分不明智的.
不过我们在NEMU中也没打算实现保护机制, 因此分段机制的各种概念, 我们也不会加入到NEMU中来. -->
In NEMU, what do we need to understand? Nothing.
Most modern operating systems no longer use the segmentation mechanism, and even the i386 manual mentions ways to "bypass" it to improve performance:
Set the segment base address to 0 and the length to 4GB, so it seems as if there is no concept of segments. This is what the i386 manual refers to as "flat mode."
Of course, "bypassing" here does not mean simply turning off the segmentation mechanism (which is actually not possible).
The concept of privilege levels mentioned in PA3 regarding the i386 protection mechanism is actually provided by the i386 segmentation mechanism, and it would be unwise to discard it.
However, we don't plan to implement the protection mechanism in NEMU, so we won't be incorporating various concepts of the segmentation mechanism into NEMU either.
